<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="author" content="Aritra Ghosh">
    <meta property="og:title" content="on model interpretability">
    <meta property="og:description" content="my insights on model interpretability after building a model probe for reasoners">
    <meta property="og:type" content="article">
    <title>on model interpretabilitys</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="../../index.html">home</a></li>
            <li><a href="projects.html">projects</a></li>
            <li><a href="../thoughts/thoughts.html">thoughts</a></li>
            <li><a href="../gallery.html">gallery</a></li>
            <li><a href="../../resume.pdf" target = "_blank">resume</a></li>
        </ul>
    </nav>

    <article class="post-container">
        <header class="post-header">
            <h1 class="post-title">on model interpretability üåê</h1>
            <p class="post-date">February 17, 2025</p>
        </header>
        
        <div class="post-content">
            <p>[update March 10, 2025 : OpenAI just released a <a href="https://openai.com/index/chain-of-thought-monitoring/" class="underlined-link">paper</a> on CoT monitoring! their approach is alignment-centric, while mine is focused on damage mitigation - but I think it‚Äôs pretty cool to see how CoT monitoring is being approached by others - would recommend giving the paper a read.]
            
            <p>when i found out that DeepSeek‚Äôs R1 API gives users access to the reasoning model‚Äôs internal CoT reasoning, i knew that i had to experiment with it. that culminated in me building a <a href="https://github.com/arighosh05/model-probe" class="underlined-link">model probe</a>, a CoT monitoring system that intercepts and analyzes CoT monitoring, alerting the relevant authorities when model misalignment is detected.</p>
            
            <p>in this blog, i‚Äôm going to share the insights i learnt while working on this project. for more info on the model probe, check out the <a href="https://github.com/arighosh05/model-probe/blob/main/README.md" class="underlined-link">README.md</a> in the project repository.</p>
            
            <p>model interpretability involves comprehending how a model operates and clearly presenting the underlying rationale behind its decision-making in ways humans can easily grasp. the absence of interpretability in the frontier models of today is frequently cited as a major barrier to broader ai adoption.  </p>

            <p>the argument in favor of model interpretability is indeed well-founded. it's difficult to envision embracing a world in which opaque, oracular black-boxes dictate decisions for us‚Äîno matter how compelling the numbers might be in demonstrating their superiority at specific tasks. The core issue isn't necessarily performance but trust. without interpretability, we're left to blindly rely on decisions whose rationale we cannot assess or challenge, fundamentally limiting accountability.</p>

            <p>building the model probe made one thing clear to me: we're just scratching the surface of truly understanding how language models reason. indeed, seeing the steps of reasoning doesn't automatically translate to understanding the conceptual frameworks these models operate within. the interpretability gap is far from being closed and we have yet to build a mapping computational processes to human-understandable concepts.</p>

            <p>the model probe i built is better conceptualized as a guardrail of sorts. digging deeper, i capture the CoT reasoning through the API endpoint and subsequently run it through a sentiment analysis pipeline, an anomaly detection pipeline and a rule-based check system, aggregating the scores in a meta-classifier to decide whether to activate the alerting system or not. in short, it conducts post hoc analysis on the CoT reasoning generated and hope for the best. while i do believe this approach has some merit, especially in the context of damage mitigation in production systems, the more valuable endeavor would be shifting towards an alignment-centric training paradigm. as an example, see this paper from OpenAI.</p>

            <p>final verdict: more foundational work in interpretability remains to be done.</p>
        
            <div class="post-footer">
            </div>
          
        </div>
        
        <a href="projects.html" class="return-link">‚Üê back to all projects</a>
    </article>
    
    <main class="social-links">
        <p>
          <a href="mailto:aritraghosh534@gmail.com">email</a>
          <a href="https://www.linkedin.com/in/ghosh-aritra/">linkedin</a>
          <a href="https://github.com/arighosh05">github</a></p>
    </main>
</body>
</html>
